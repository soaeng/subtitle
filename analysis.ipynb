{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 형태소 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymongo\n",
    "\n",
    "# DB 연결\n",
    "conn = pymongo.MongoClient(\"localhost\", 27017)\n",
    "\n",
    "db = conn[\"soaeng\"]\n",
    "dbcol = db.subtitle\n",
    "\n",
    "# 파일 읽기\n",
    "def read_txt_file(file_name):\n",
    "  # print(\"[파일명] \" + file_name)\n",
    "  text = \"\"\n",
    "  \n",
    "  # 파일 열기\n",
    "  with open(file_path + file_name, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "    # 파일 라인별로 텍스트만 추출\n",
    "    for line in txt_file:\n",
    "      line = line.strip()\n",
    "      text += line\n",
    "  return text\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\sodud\\\\study\\\\ssafy_spec_pjt\\\\subtitle\\\\data\\\\refined_text\\\\\"\n",
    "\n",
    "# 자막 파일 폴더\n",
    "file_list = os.listdir(file_path)\n",
    "# print(file_list)\n",
    "# 자막 파일 목록\n",
    "txt_files = [txt_file for txt_file in file_list]\n",
    "\n",
    "# 파일 내용 저장할 리스트\n",
    "text_list = []\n",
    "\n",
    "# 파일 읽고 내용 저장\n",
    "for txt_file in txt_files:\n",
    "  text_list.append(read_txt_file(txt_file))\n",
    "  subtitle = {\"filename\": txt_file, \"content\": read_txt_file(txt_file)}\n",
    "  # DB에 저장(이미 저장된 파일이면 내용 업데이트, 없으면 삽입)\n",
    "  dbcol.update_one(subtitle, { '$set': subtitle }, upsert=True)\n",
    "\n",
    "# print(text_list)\n",
    "print(len(text_list))\n",
    "print(dbcol.estimated_document_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "conn = pymongo.MongoClient(\"localhost\", 27017)\n",
    "db = conn[\"soaeng\"]\n",
    "dbcol = db.subtitle\n",
    "\n",
    "# DB에서 얻은 정보를 데이터 프레임화\n",
    "df = pd.DataFrame(list(dbcol.find()))\n",
    "\n",
    "# content 열만 가져오기\n",
    "df_content = df.loc[:, [\"content\"]]\n",
    "\n",
    "print(df_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어:  ['あそこ', 'あっ', 'あの', 'あのかた', 'あの人', 'あり', 'あります', 'ある', 'あれ', 'い', 'いう', 'います', 'いる', 'う', 'うち', 'え', 'お', 'および', 'おり', 'おります', 'か', 'かつて', 'から', 'が', 'き', 'ここ', 'こちら', 'こと', 'この', 'これ', 'これら', 'さ', 'さらに', 'し', 'しかし', 'する', 'ず', 'せ', 'せる', 'そこ', 'そして', 'その', 'その他', 'その後', 'それ', 'それぞれ', 'それで', 'た', 'ただし', 'たち', 'ため', 'たり', 'だ', 'だっ', 'だれ', 'つ', 'て', 'で', 'でき', 'できる', 'です', 'では', 'でも', 'と', 'という', 'といった', 'とき', 'ところ', 'として', 'とともに', 'とも', 'と共に', 'どこ', 'どの', 'な', 'ない', 'なお', 'なかっ', 'ながら', 'なく', 'なっ', 'など', 'なに', 'なら', 'なり', 'なる', 'なん', 'に', 'において', 'における', 'について', 'にて', 'によって', 'により', 'による', 'に対して', 'に対する', 'に関する', 'の', 'ので', 'のみ', 'は', 'ば', 'へ', 'ほか', 'ほとんど', 'ほど', 'ます', 'また', 'または', 'まで', 'も', 'もの', 'ものの', 'や', 'よう', 'より', 'ら', 'られ', 'られる', 'れ', 'れる', 'を', 'ん', '何', '及び', '彼', '彼女', '我々', '特に', '私', '私達', '貴方', '貴方方']\n"
     ]
    }
   ],
   "source": [
    "# 불용어 리스트 생성\n",
    "stop_words = []\n",
    "with open(\"stop_words_japanese.txt\", \"r\", encoding=\"UTF8\") as f :\n",
    "  lines = f.readlines()\n",
    "  stop_words = [stop_word.strip() for stop_word in lines]\n",
    "\n",
    "# 생성된 불용어 리스트 확인\n",
    "print(\"불용어: \", stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import MeCab\n",
    "\n",
    "sentence = \"\"\n",
    "file_path = \"C:\\\\Users\\\\sodud\\\\study\\\\ssafy_spec_pjt\\\\subtitle\\\\data\\\\refined_text\\\\\"\n",
    "file_name = \"Ni_Ten_Yonsan_Seiin_Koukou_Danshi_Volley-bu_001.txt\"\n",
    "text = open(file_path + file_name, \"r\", encoding=\"UTF8\")\n",
    "for line in text:\n",
    "  sentence += line\n",
    "  \n",
    "try:\n",
    "  # 품사 분석\n",
    "  # tagger = MeCab.Tagger()\n",
    "  # tagger = MeCab.Tagger(\"-O wakati\")\n",
    "  tagger = MeCab.Tagger(\"-O wakati -F%m\\\\t --unk-feature 未知語\")\n",
    "  \n",
    "  # 기본 출력 결과\n",
    "  # print(tagger.parse(sentence))\n",
    "  \n",
    "  # 토큰화\n",
    "  m = tagger.parseToNode(sentence)\n",
    "  while m:\n",
    "    feature = m.feature.split(\",\")\n",
    "    if feature[0] != \"未知語\":\n",
    "      print(m.surface, \"\\t\", feature[7].split(\"-\")[0])\n",
    "    m = m.next\n",
    "  print(\"EOS\")\n",
    "  \n",
    "  # 일본어 사전 정보 출력\n",
    "  d = tagger.dictionary_info()\n",
    "  while d:\n",
    "    print(\"filename: %s\" % d.filename)\n",
    "    print(\"charset: %s\" %  d.charset)\n",
    "    print(\"size: %d\" %  d.size)\n",
    "    print(\"type: %d\" %  d.type)\n",
    "    print(\"lsize: %d\" %  d.lsize)\n",
    "    print(\"rsize: %d\" %  d.rsize)\n",
    "    print(\"version: %d\" %  d.version)\n",
    "    d = d.next\n",
    "\n",
    "\n",
    "except RuntimeError as e:\n",
    "  print(\"RuntimeError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ 일본어 불용어 목록을 위한 라이브러리 설치\n",
    "단, 수동으로 불용어 목록 추가 필요(일본어 지원 X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sodud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import neologdn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']\n",
    "\n",
    "tagger = MeCab.Tagger(\"-F%m\\\\t --unk-feature 未知語\")\n",
    "\n",
    "# 포함 품사: 명사, 동사, 형용사, 부사\n",
    "CONTENT_WORD_POS = (\"名詞\", \"動詞\", \"形容詞\", \"副詞\")\n",
    "# 제외 품사: 접미사, 비자립, 대명사, 미지어\n",
    "IGNORE = (\"接尾\", \"非自立\", \"代名詞\", \"未知語\")\n",
    "\n",
    "def is_content_word(feature):\n",
    "  return feature.startswith(CONTENT_WORD_POS) and all(f not in IGNORE for f in feature.split(\",\")[:6])\n",
    "\n",
    "for i in text_list:\n",
    "  text += i\n",
    "\n",
    "text = neologdn.normalize(text, repeat=2)\n",
    "\n",
    "#some more noise removal\n",
    "text = \"\".join([i for i in text if i.isalpha() or i.isspace()])\n",
    "result = tagger.parseToNode(text)\n",
    "print(result)\n",
    "\n",
    "stop_words = stopwords.words(\"japanese\")\n",
    "\n",
    "content_words = []\n",
    "\n",
    "while result:\n",
    "  if is_content_word(result.feature):\n",
    "    # print(result.surface + \"\\t\" + result.feature)\n",
    "    lemma = result.feature.split(\",\")[10] if len(result.feature.split(\",\")) > 6 and result.feature.split(\",\")[6] != \"*\" else result.surface\n",
    "    if lemma not in stop_words:\n",
    "      content_words.append((lemma))\n",
    "  result = result.next\n",
    "\n",
    "print(content_words)\n",
    "fdist = FreqDist(content_words)\n",
    "fdist.plot(10, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ 빈도수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 포함 품사: 명사, 동사, 형용사, 부사\n",
    "CONTENT_WORD_POS = (\"名詞\", \"動詞\", \"形容詞\", \"副詞\")\n",
    "# 제외 품사: 접미사, 비자립, 대명사, 미지어\n",
    "IGNORE = (\"接尾\", \"非自立\", \"代名詞\", \"未知語\")\n",
    "\n",
    "def is_content_word(feature):\n",
    "    return feature.startswith(CONTENT_WORD_POS) and all(f not in IGNORE for f in feature.split(\",\")[:6])\n",
    "\n",
    "content_words = []\n",
    "\n",
    "for text in text_list:\n",
    "  \n",
    "  # 토큰화\n",
    "  m = tagger.parseToNode(text)\n",
    "  \n",
    "  while m:\n",
    "    # 해당되는 단어만 뽑기\n",
    "    if is_content_word(m.feature):\n",
    "      word = m.feature.split(\",\")[7] if len(m.feature.split(\",\")) > 6 and m.feature.split(\",\")[6] != \"*\" else m.surface\n",
    "      if word not in stop_words:\n",
    "        content_words.append((word.split(\"-\")[0]))\n",
    "    m = m.next\n",
    "\n",
    "print(content_words)\n",
    "# 빈도수 계산\n",
    "count = Counter(content_words)\n",
    "\n",
    "# 단어와 빈도수 딕셔너리 저장 리스트\n",
    "word_count = []\n",
    "words = []\n",
    "\n",
    "for word, cnt in count.most_common():\n",
    "  dics = {\"word\": word, \"count\": cnt}\n",
    "  if len(dics[\"word\"]) >= 2 and len(words) <= 99:\n",
    "    word_count.append(dics)\n",
    "    words.append(dics[\"word\"])\n",
    "\n",
    "# 단어, 빈도수 출력\n",
    "# for word in word_count:\n",
    "#   print(\"{}\\t{}\".format(word[\"word\"], word[\"count\"]))\n",
    "\n",
    "# print(word_count)\n",
    "# print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for i in word_count:\n",
    "  # print(i[\"count\"])\n",
    "  counts.append(i[\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ 데이터 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# 폰트 경로 설정\n",
    "font_path = \"C:/Users/sodud/study/ssafy_spec_pjt/subtitle/MPLUSRounded1c-Regular.ttf\"\n",
    "\n",
    "# 딕셔너리화\n",
    "def pairing_word_cnt(word_count):\n",
    "  wordInfo = dict()\n",
    "  for i in word_count:\n",
    "    wordInfo[i[\"word\"]] = i[\"count\"]\n",
    "  return wordInfo\n",
    "\n",
    "# 데이터 시각화(막대 그래프)\n",
    "def show_bar_chart(font_path, data):\n",
    "  # 폰트 설정\n",
    "  font_name = fm.FontProperties(fname=\"C:/Windows/Fonts/msgothic.ttc\").get_name()\n",
    "  plt.rc(\"font\", family=font_name)\n",
    "  \n",
    "  # 30개씩의 데이터만\n",
    "  x = list(data.keys())[:30]\n",
    "  y = list(data.values())[:30]\n",
    "  \n",
    "  # 그래프 비율\n",
    "  ax = plt.subplot(3, 1, 1)\n",
    "  \n",
    "  # 막대 설정\n",
    "  plt.bar(x, y, width=.6)\n",
    "  plt.grid(True)\n",
    "  # x축 레이블 설정\n",
    "  plt.xticks(fontsize=8, rotation=60)\n",
    "  \n",
    "  # 그래프 이미지 사이즈 설정\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  plt.show()\n",
    "\n",
    "# 데이터 시각화(워드 클리우드)\n",
    "def show_wordcloud(font_path, data):\n",
    "  # wordcloud = WordCloud(font_path).generate(\" \".join(data))\n",
    "  wordcloud = WordCloud(font_path).generate(\" \".join(data))\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "  plt.show()\n",
    "\n",
    "show_wordcloud(font_path, words)\n",
    "show_bar_chart(font_path, pairing_word_cnt(word_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eebaf1173d8d9c3c4ee9a7b8bb1432a7f576348d6cb7a26bc263375fbc310797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
