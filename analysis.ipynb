{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ í˜•íƒœì†Œ ë¶„ì„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ íŒŒì¼ ì½ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymongo\n",
    "\n",
    "# DB ì—°ê²°\n",
    "conn = pymongo.MongoClient(\"localhost\", 27017)\n",
    "\n",
    "db = conn[\"soaeng\"]\n",
    "dbcol = db.subtitle\n",
    "\n",
    "# íŒŒì¼ ì½ê¸°\n",
    "def read_txt_file(file_name):\n",
    "  # print(\"[íŒŒì¼ëª…] \" + file_name)\n",
    "  text = \"\"\n",
    "  \n",
    "  # íŒŒì¼ ì—´ê¸°\n",
    "  with open(file_path + file_name, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "    # íŒŒì¼ ë¼ì¸ë³„ë¡œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
    "    for line in txt_file:\n",
    "      line = line.strip()\n",
    "      text += line\n",
    "  return text\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\sodud\\\\study\\\\ssafy_spec_pjt\\\\subtitle\\\\data\\\\refined_text\\\\\"\n",
    "\n",
    "# ìžë§‰ íŒŒì¼ í´ë”\n",
    "file_list = os.listdir(file_path)\n",
    "# print(file_list)\n",
    "# ìžë§‰ íŒŒì¼ ëª©ë¡\n",
    "txt_files = [txt_file for txt_file in file_list]\n",
    "\n",
    "# íŒŒì¼ ë‚´ìš© ì €ìž¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "text_list = []\n",
    "\n",
    "# íŒŒì¼ ì½ê³  ë‚´ìš© ì €ìž¥\n",
    "for txt_file in txt_files:\n",
    "  text_list.append(read_txt_file(txt_file))\n",
    "  subtitle = {\"filename\": txt_file, \"content\": read_txt_file(txt_file)}\n",
    "  # DBì— ì €ìž¥(ì´ë¯¸ ì €ìž¥ëœ íŒŒì¼ì´ë©´ ë‚´ìš© ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ìž…)\n",
    "  dbcol.update_one(subtitle, { '$set': subtitle }, upsert=True)\n",
    "\n",
    "# print(text_list)\n",
    "print(len(text_list))\n",
    "print(dbcol.estimated_document_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "conn = pymongo.MongoClient(\"localhost\", 27017)\n",
    "db = conn[\"soaeng\"]\n",
    "dbcol = db.subtitle\n",
    "\n",
    "# DBì—ì„œ ì–»ì€ ì •ë³´ë¥¼ ë°ì´í„° í”„ë ˆìž„í™”\n",
    "df = pd.DataFrame(list(dbcol.find()))\n",
    "\n",
    "# content ì—´ë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    "df_content = df.loc[:, [\"content\"]]\n",
    "\n",
    "print(df_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ë¶ˆìš©ì–´ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¶ˆìš©ì–´:  ['ã‚ãã“', 'ã‚ã£', 'ã‚ã®', 'ã‚ã®ã‹ãŸ', 'ã‚ã®äºº', 'ã‚ã‚Š', 'ã‚ã‚Šã¾ã™', 'ã‚ã‚‹', 'ã‚ã‚Œ', 'ã„', 'ã„ã†', 'ã„ã¾ã™', 'ã„ã‚‹', 'ã†', 'ã†ã¡', 'ãˆ', 'ãŠ', 'ãŠã‚ˆã³', 'ãŠã‚Š', 'ãŠã‚Šã¾ã™', 'ã‹', 'ã‹ã¤ã¦', 'ã‹ã‚‰', 'ãŒ', 'ã', 'ã“ã“', 'ã“ã¡ã‚‰', 'ã“ã¨', 'ã“ã®', 'ã“ã‚Œ', 'ã“ã‚Œã‚‰', 'ã•', 'ã•ã‚‰ã«', 'ã—', 'ã—ã‹ã—', 'ã™ã‚‹', 'ãš', 'ã›', 'ã›ã‚‹', 'ãã“', 'ãã—ã¦', 'ãã®', 'ãã®ä»–', 'ãã®å¾Œ', 'ãã‚Œ', 'ãã‚Œãžã‚Œ', 'ãã‚Œã§', 'ãŸ', 'ãŸã ã—', 'ãŸã¡', 'ãŸã‚', 'ãŸã‚Š', 'ã ', 'ã ã£', 'ã ã‚Œ', 'ã¤', 'ã¦', 'ã§', 'ã§ã', 'ã§ãã‚‹', 'ã§ã™', 'ã§ã¯', 'ã§ã‚‚', 'ã¨', 'ã¨ã„ã†', 'ã¨ã„ã£ãŸ', 'ã¨ã', 'ã¨ã“ã‚', 'ã¨ã—ã¦', 'ã¨ã¨ã‚‚ã«', 'ã¨ã‚‚', 'ã¨å…±ã«', 'ã©ã“', 'ã©ã®', 'ãª', 'ãªã„', 'ãªãŠ', 'ãªã‹ã£', 'ãªãŒã‚‰', 'ãªã', 'ãªã£', 'ãªã©', 'ãªã«', 'ãªã‚‰', 'ãªã‚Š', 'ãªã‚‹', 'ãªã‚“', 'ã«', 'ã«ãŠã„ã¦', 'ã«ãŠã‘ã‚‹', 'ã«ã¤ã„ã¦', 'ã«ã¦', 'ã«ã‚ˆã£ã¦', 'ã«ã‚ˆã‚Š', 'ã«ã‚ˆã‚‹', 'ã«å¯¾ã—ã¦', 'ã«å¯¾ã™ã‚‹', 'ã«é–¢ã™ã‚‹', 'ã®', 'ã®ã§', 'ã®ã¿', 'ã¯', 'ã°', 'ã¸', 'ã»ã‹', 'ã»ã¨ã‚“ã©', 'ã»ã©', 'ã¾ã™', 'ã¾ãŸ', 'ã¾ãŸã¯', 'ã¾ã§', 'ã‚‚', 'ã‚‚ã®', 'ã‚‚ã®ã®', 'ã‚„', 'ã‚ˆã†', 'ã‚ˆã‚Š', 'ã‚‰', 'ã‚‰ã‚Œ', 'ã‚‰ã‚Œã‚‹', 'ã‚Œ', 'ã‚Œã‚‹', 'ã‚’', 'ã‚“', 'ä½•', 'åŠã³', 'å½¼', 'å½¼å¥³', 'æˆ‘ã€…', 'ç‰¹ã«', 'ç§', 'ç§é”', 'è²´æ–¹', 'è²´æ–¹æ–¹']\n"
     ]
    }
   ],
   "source": [
    "# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ ìƒì„±\n",
    "stop_words = []\n",
    "with open(\"stop_words_japanese.txt\", \"r\", encoding=\"UTF8\") as f :\n",
    "  lines = f.readlines()\n",
    "  stop_words = [stop_word.strip() for stop_word in lines]\n",
    "\n",
    "# ìƒì„±ëœ ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ í™•ì¸\n",
    "print(\"ë¶ˆìš©ì–´: \", stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ í˜•íƒœì†Œ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import MeCab\n",
    "\n",
    "sentence = \"\"\n",
    "file_path = \"C:\\\\Users\\\\sodud\\\\study\\\\ssafy_spec_pjt\\\\subtitle\\\\data\\\\refined_text\\\\\"\n",
    "file_name = \"Ni_Ten_Yonsan_Seiin_Koukou_Danshi_Volley-bu_001.txt\"\n",
    "text = open(file_path + file_name, \"r\", encoding=\"UTF8\")\n",
    "for line in text:\n",
    "  sentence += line\n",
    "  \n",
    "try:\n",
    "  # í’ˆì‚¬ ë¶„ì„\n",
    "  # tagger = MeCab.Tagger()\n",
    "  # tagger = MeCab.Tagger(\"-O wakati\")\n",
    "  tagger = MeCab.Tagger(\"-O wakati -F%m\\\\t --unk-feature æœªçŸ¥èªž\")\n",
    "  \n",
    "  # ê¸°ë³¸ ì¶œë ¥ ê²°ê³¼\n",
    "  # print(tagger.parse(sentence))\n",
    "  \n",
    "  # í† í°í™”\n",
    "  m = tagger.parseToNode(sentence)\n",
    "  while m:\n",
    "    feature = m.feature.split(\",\")\n",
    "    if feature[0] != \"æœªçŸ¥èªž\":\n",
    "      print(m.surface, \"\\t\", feature[7].split(\"-\")[0])\n",
    "    m = m.next\n",
    "  print(\"EOS\")\n",
    "  \n",
    "  # ì¼ë³¸ì–´ ì‚¬ì „ ì •ë³´ ì¶œë ¥\n",
    "  d = tagger.dictionary_info()\n",
    "  while d:\n",
    "    print(\"filename: %s\" % d.filename)\n",
    "    print(\"charset: %s\" %  d.charset)\n",
    "    print(\"size: %d\" %  d.size)\n",
    "    print(\"type: %d\" %  d.type)\n",
    "    print(\"lsize: %d\" %  d.lsize)\n",
    "    print(\"rsize: %d\" %  d.rsize)\n",
    "    print(\"version: %d\" %  d.version)\n",
    "    d = d.next\n",
    "\n",
    "\n",
    "except RuntimeError as e:\n",
    "  print(\"RuntimeError:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ì¼ë³¸ì–´ ë¶ˆìš©ì–´ ëª©ë¡ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "ë‹¨, ìˆ˜ë™ìœ¼ë¡œ ë¶ˆìš©ì–´ ëª©ë¡ ì¶”ê°€ í•„ìš”(ì¼ë³¸ì–´ ì§€ì› X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sodud\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import neologdn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.sans-serif'] = ['Hiragino Maru Gothic Pro', 'Yu Gothic', 'Meirio', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP']\n",
    "\n",
    "tagger = MeCab.Tagger(\"-F%m\\\\t --unk-feature æœªçŸ¥èªž\")\n",
    "\n",
    "# í¬í•¨ í’ˆì‚¬: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬\n",
    "CONTENT_WORD_POS = (\"åè©ž\", \"å‹•è©ž\", \"å½¢å®¹è©ž\", \"å‰¯è©ž\")\n",
    "# ì œì™¸ í’ˆì‚¬: ì ‘ë¯¸ì‚¬, ë¹„ìžë¦½, ëŒ€ëª…ì‚¬, ë¯¸ì§€ì–´\n",
    "IGNORE = (\"æŽ¥å°¾\", \"éžè‡ªç«‹\", \"ä»£åè©ž\", \"æœªçŸ¥èªž\")\n",
    "\n",
    "def is_content_word(feature):\n",
    "  return feature.startswith(CONTENT_WORD_POS) and all(f not in IGNORE for f in feature.split(\",\")[:6])\n",
    "\n",
    "for i in text_list:\n",
    "  text += i\n",
    "\n",
    "text = neologdn.normalize(text, repeat=2)\n",
    "\n",
    "#some more noise removal\n",
    "text = \"\".join([i for i in text if i.isalpha() or i.isspace()])\n",
    "result = tagger.parseToNode(text)\n",
    "print(result)\n",
    "\n",
    "stop_words = stopwords.words(\"japanese\")\n",
    "\n",
    "content_words = []\n",
    "\n",
    "while result:\n",
    "  if is_content_word(result.feature):\n",
    "    # print(result.surface + \"\\t\" + result.feature)\n",
    "    lemma = result.feature.split(\",\")[10] if len(result.feature.split(\",\")) > 6 and result.feature.split(\",\")[6] != \"*\" else result.surface\n",
    "    if lemma not in stop_words:\n",
    "      content_words.append((lemma))\n",
    "  result = result.next\n",
    "\n",
    "print(content_words)\n",
    "fdist = FreqDist(content_words)\n",
    "fdist.plot(10, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ë¹ˆë„ìˆ˜ ê³„ì‚°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# í¬í•¨ í’ˆì‚¬: ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ë¶€ì‚¬\n",
    "CONTENT_WORD_POS = (\"åè©ž\", \"å‹•è©ž\", \"å½¢å®¹è©ž\", \"å‰¯è©ž\")\n",
    "# ì œì™¸ í’ˆì‚¬: ì ‘ë¯¸ì‚¬, ë¹„ìžë¦½, ëŒ€ëª…ì‚¬, ë¯¸ì§€ì–´\n",
    "IGNORE = (\"æŽ¥å°¾\", \"éžè‡ªç«‹\", \"ä»£åè©ž\", \"æœªçŸ¥èªž\")\n",
    "\n",
    "def is_content_word(feature):\n",
    "    return feature.startswith(CONTENT_WORD_POS) and all(f not in IGNORE for f in feature.split(\",\")[:6])\n",
    "\n",
    "content_words = []\n",
    "\n",
    "for text in text_list:\n",
    "  \n",
    "  # í† í°í™”\n",
    "  m = tagger.parseToNode(text)\n",
    "  \n",
    "  while m:\n",
    "    # í•´ë‹¹ë˜ëŠ” ë‹¨ì–´ë§Œ ë½‘ê¸°\n",
    "    if is_content_word(m.feature):\n",
    "      word = m.feature.split(\",\")[7] if len(m.feature.split(\",\")) > 6 and m.feature.split(\",\")[6] != \"*\" else m.surface\n",
    "      if word not in stop_words:\n",
    "        content_words.append((word.split(\"-\")[0]))\n",
    "    m = m.next\n",
    "\n",
    "print(content_words)\n",
    "# ë¹ˆë„ìˆ˜ ê³„ì‚°\n",
    "count = Counter(content_words)\n",
    "\n",
    "# ë‹¨ì–´ì™€ ë¹ˆë„ìˆ˜ ë”•ì…”ë„ˆë¦¬ ì €ìž¥ ë¦¬ìŠ¤íŠ¸\n",
    "word_count = []\n",
    "words = []\n",
    "\n",
    "for word, cnt in count.most_common():\n",
    "  dics = {\"word\": word, \"count\": cnt}\n",
    "  if len(dics[\"word\"]) >= 2 and len(words) <= 99:\n",
    "    word_count.append(dics)\n",
    "    words.append(dics[\"word\"])\n",
    "\n",
    "# ë‹¨ì–´, ë¹ˆë„ìˆ˜ ì¶œë ¥\n",
    "# for word in word_count:\n",
    "#   print(\"{}\\t{}\".format(word[\"word\"], word[\"count\"]))\n",
    "\n",
    "# print(word_count)\n",
    "# print(words)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for i in word_count:\n",
    "  # print(i[\"count\"])\n",
    "  counts.append(i[\"word\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ë°ì´í„° ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# í°íŠ¸ ê²½ë¡œ ì„¤ì •\n",
    "font_path = \"C:/Users/sodud/study/ssafy_spec_pjt/subtitle/MPLUSRounded1c-Regular.ttf\"\n",
    "\n",
    "# ë”•ì…”ë„ˆë¦¬í™”\n",
    "def pairing_word_cnt(word_count):\n",
    "  wordInfo = dict()\n",
    "  for i in word_count:\n",
    "    wordInfo[i[\"word\"]] = i[\"count\"]\n",
    "  return wordInfo\n",
    "\n",
    "# ë°ì´í„° ì‹œê°í™”(ë§‰ëŒ€ ê·¸ëž˜í”„)\n",
    "def show_bar_chart(font_path, data):\n",
    "  # í°íŠ¸ ì„¤ì •\n",
    "  font_name = fm.FontProperties(fname=\"C:/Windows/Fonts/msgothic.ttc\").get_name()\n",
    "  plt.rc(\"font\", family=font_name)\n",
    "  \n",
    "  # 30ê°œì”©ì˜ ë°ì´í„°ë§Œ\n",
    "  x = list(data.keys())[:30]\n",
    "  y = list(data.values())[:30]\n",
    "  \n",
    "  # ê·¸ëž˜í”„ ë¹„ìœ¨\n",
    "  ax = plt.subplot(3, 1, 1)\n",
    "  \n",
    "  # ë§‰ëŒ€ ì„¤ì •\n",
    "  plt.bar(x, y, width=.6)\n",
    "  plt.grid(True)\n",
    "  # xì¶• ë ˆì´ë¸” ì„¤ì •\n",
    "  plt.xticks(fontsize=8, rotation=60)\n",
    "  \n",
    "  # ê·¸ëž˜í”„ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ ì„¤ì •\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  plt.show()\n",
    "\n",
    "# ë°ì´í„° ì‹œê°í™”(ì›Œë“œ í´ë¦¬ìš°ë“œ)\n",
    "def show_wordcloud(font_path, data):\n",
    "  # wordcloud = WordCloud(font_path).generate(\" \".join(data))\n",
    "  wordcloud = WordCloud(font_path).generate(\" \".join(data))\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "  plt.show()\n",
    "\n",
    "show_wordcloud(font_path, words)\n",
    "show_bar_chart(font_path, pairing_word_cnt(word_count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eebaf1173d8d9c3c4ee9a7b8bb1432a7f576348d6cb7a26bc263375fbc310797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
