{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📌 자막 데이터 크롤링  \n",
    "* urllib 라이브러리 사용해 파일 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ 애니메이션 자막 페이지 목록 받아오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 읽어들이기\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# 파일 주소 크롤링\n",
    "def bs_parsing(url):\n",
    "  response = requests.get(root_url + url)\n",
    "  soup = bs(response.text, \"html.parser\")\n",
    "  elements = soup.select(\"table#flisttable td > a\")\n",
    "  \n",
    "  # 주소 담을 리스트\n",
    "  page_list = []\n",
    "  \n",
    "  # 상세 페이지 주소 리스트화\n",
    "  for (idx, el) in enumerate(elements, 1):\n",
    "    page_list.append(root_url + el.attrs[\"href\"].replace(\"%2F\", \"/\"))\n",
    "    \n",
    "  return page_list\n",
    "\n",
    "# 리스트 값을 txt 파일에 한 줄씩 저장\n",
    "def save_text_file(page_list):\n",
    "  f = open(\"C:/Users/sodud/study/ssafy_spec_pjt/subtitle/data/page_url_list.txt\", \"w\", encoding=\"UTF8\")\n",
    "  for page in page_list:\n",
    "    f.write(page)\n",
    "    f.write(\"\\n\")\n",
    "  f.close\n",
    "  (\"[SUCCESS] 자막 페이지 목록 저장\")\n",
    "\n",
    "# 자막 사이트\n",
    "root_url = \"https://kitsunekko.net\"\n",
    "\n",
    "# 일본어 자막 목록 페이지\n",
    "page_url = \"/dirlist.php?dir=subtitles/japanese/\"\n",
    "\n",
    "# 자막 페이지 목록 리스트로 저장\n",
    "page_list = bs_parsing(page_url)\n",
    "\n",
    "# 리스트를 txt 파일로 저장\n",
    "save_text_file(page_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ✏️ srt 확장자 파일만 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import os\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from inspect import getfile\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "def read_text_file(page_list_file, start, unit):\n",
    "  flag = True\n",
    "  f = open(page_list_file, \"r\", encoding=\"UTF8\")\n",
    "  lines = f.readlines()[start: start+unit]\n",
    "  \n",
    "  if len(lines) == 0:\n",
    "    return\n",
    "  \n",
    "  if len(lines) < unit:\n",
    "    flag = False\n",
    "  \n",
    "  for line in lines:\n",
    "    \n",
    "    # 주소가 아니면 pass\n",
    "    if line[-3:] != \"%2F\":\n",
    "      pass\n",
    "    \n",
    "    response = requests.get(line[:-1])\n",
    "    soup = bs(response.text, \"html.parser\")\n",
    "    files = soup.select(\"table#flisttable td > a\")\n",
    "    \n",
    "    # 파일 주소 담을 리스트\n",
    "    file_urls = []\n",
    "    \n",
    "    # 파일 읽기\n",
    "    for f in files:\n",
    "      file_path= f.attrs[\"href\"]\n",
    "      \n",
    "      # srt 파일만 저장\n",
    "      if file_path[-3:] == \"srt\":\n",
    "        file_urls.append(\"/\" + file_path)\n",
    "      else:\n",
    "        pass\n",
    "    \n",
    "    # 자막 페이지 확인\n",
    "    print(file_path)\n",
    "    \n",
    "    # 파일이 존재하는지 검색\n",
    "    file_search(file_urls)\n",
    "    \n",
    "  f.close\n",
    "  \n",
    "  if flag == False:\n",
    "    return\n",
    "  \n",
    "  return read_text_file(page_list_file, start + unit, unit)\n",
    "\n",
    "# 파일이 존재하는지 검색\n",
    "def file_search(file_urls):\n",
    "  for file_url in file_urls:\n",
    "    # 파일 이름만 추출\n",
    "    depth = file_url.split(\"/\")\n",
    "    filename = depth[len(depth)-1]\n",
    "    try:\n",
    "      # 파일 저장 경로\n",
    "      path = \"C:\\\\Users\\\\sodud\\\\study\\\\ssafy_spec_pjt\\\\subtitle\\\\data\\\\text\\\\\"\n",
    "      \n",
    "      # 파일이 저장되어 있지 않으면 다운로드\n",
    "      if os.path.isfile(path + filename):\n",
    "        print(\"이미 존재하는 파일: \" + filename)\n",
    "      else:\n",
    "        get_download(root_url + file_url.replace(\" \", \"%20\"), filename, path)\n",
    "        \n",
    "    except HTTPError as err:\n",
    "      print(\"다운로드 중 에러 발생: \" + filename + \"/\" + err)\n",
    "      pass\n",
    "\n",
    "# 파일 다운로드\n",
    "\"\"\"\n",
    "[*]   url: 파일 다운로드 경로\n",
    "[*] fname: 저장할 파일명\n",
    "[*]   dir: 저장 폴더 경로\n",
    "\"\"\"\n",
    "def get_download(url, fname, dir):\n",
    "  try:\n",
    "    os.chdir(dir)\n",
    "    # 다운로드 요청\n",
    "    request.urlretrieve(url, fname)\n",
    "    print(\"다운로드 완료: \" + fname)\n",
    "  except HTTPError as err:\n",
    "    print(\"에러 발생:\" + err)\n",
    "    return\n",
    "\n",
    "# 자막 사이트\n",
    "root_url = \"https://kitsunekko.net\"\n",
    "\n",
    "# 자막 파일 페이지 목록 파일\n",
    "page_list_file = \"C:/Users/sodud/study/ssafy_spec_pjt/subtitle/data/page_url_list.txt\"\n",
    "\n",
    "# 목록 파일 읽기\n",
    "read_text_file(page_list_file, 0, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "eebaf1173d8d9c3c4ee9a7b8bb1432a7f576348d6cb7a26bc263375fbc310797"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
